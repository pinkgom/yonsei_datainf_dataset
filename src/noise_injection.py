import random
import numpy as np
import pandas as pd
import re
from typing import Dict, List, Tuple
import copy


class NoiseInjector:
    def __init__(self, random_seed=42):
        """
        ë…¸ì´ì¦ˆ ì£¼ì…ê¸° ì´ˆê¸°í™”
        """
        random.seed(random_seed)
        np.random.seed(random_seed)

        # ì¼ë°˜ì ì¸ ì² ì ì˜¤ë¥˜ íŒ¨í„´ë“¤
        self.common_typos = {
            'the': ['teh', 'hte', 'te'],
            'and': ['adn', 'nad', 'an'],
            'you': ['yuo', 'yu', 'oyu'],
            'for': ['fro', 'ofr', 'fo'],
            'are': ['aer', 'rae', 'ar'],
            'with': ['wiht', 'whit', 'wit'],
            'this': ['thsi', 'tihs', 'ths']
        }

    def inject_noise(self, df: pd.DataFrame, noise_ratio: float = 0.2) -> Tuple[pd.DataFrame, List[int]]:
        """
        ë°ì´í„°í”„ë ˆì„ì— ë…¸ì´ì¦ˆ ì£¼ì…

        Args:
            df: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
            noise_ratio: ë…¸ì´ì¦ˆë¥¼ ì£¼ì…í•  ë¹„ìœ¨ (0.0 ~ 1.0)

        Returns:
            noisy_df: ë…¸ì´ì¦ˆê°€ ì£¼ì…ëœ ë°ì´í„°í”„ë ˆì„
            noisy_indices: ë…¸ì´ì¦ˆê°€ ì£¼ì…ëœ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n=== ë…¸ì´ì¦ˆ ì£¼ì… ì‹œì‘ (ë¹„ìœ¨: {noise_ratio * 100}%) ===")

        noisy_df = df.copy()
        n_samples = len(df)
        n_noisy = int(n_samples * noise_ratio)

        # ë…¸ì´ì¦ˆë¥¼ ì£¼ì…í•  ìƒ˜í”Œ ì¸ë±ìŠ¤ ëœë¤ ì„ íƒ
        noisy_indices = random.sample(range(n_samples), n_noisy)

        print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {n_samples}")
        print(f"ë…¸ì´ì¦ˆ ì£¼ì… ëŒ€ìƒ: {n_noisy}ê°œ")

        # ë…¸ì´ì¦ˆ ìœ í˜•ë³„ ì¹´ìš´í„°
        noise_type_counts = {"grammar": 0, "semantic": 0, "quality": 0}

        for idx in noisy_indices:
            # ë…¸ì´ì¦ˆ ìœ í˜•ì„ ëœë¤í•˜ê²Œ ì„ íƒ
            noise_type = random.choice(['grammar', 'semantic', 'quality'])
            noise_type_counts[noise_type] += 1

            # ì„ íƒëœ ë…¸ì´ì¦ˆ ìœ í˜•ì— ë”°ë¼ ë…¸ì´ì¦ˆ ì£¼ì…
            if noise_type == 'grammar':
                noisy_df.iloc[idx] = self._apply_grammar_noise(df.iloc[idx])
            elif noise_type == 'semantic':
                noisy_df.iloc[idx] = self._apply_semantic_noise(df.iloc[idx], df)
            elif noise_type == 'quality':
                noisy_df.iloc[idx] = self._apply_quality_noise(df.iloc[idx])

        print("ë…¸ì´ì¦ˆ ìœ í˜•ë³„ ì ìš© ê°œìˆ˜:")
        for noise_type, count in noise_type_counts.items():
            print(f"  - {noise_type}: {count}ê°œ")

        return noisy_df, noisy_indices

    def _apply_grammar_noise(self, sample: pd.Series) -> pd.Series:
        """ë¬¸ë²•/ì² ì ì˜¤ë¥˜ ë…¸ì´ì¦ˆ ì£¼ì…"""
        noisy_sample = sample.copy()

        # 50% í™•ë¥ ë¡œ instruction ë˜ëŠ” outputì— ë…¸ì´ì¦ˆ ì£¼ì…
        target_field = random.choice(['instruction', 'output'])
        text = str(noisy_sample[target_field])

        # ë…¸ì´ì¦ˆ ìœ í˜• ì„ íƒ
        grammar_noise_type = random.choice(['typo', 'word_order', 'punctuation'])

        if grammar_noise_type == 'typo':
            # ì² ì ì˜¤ë¥˜ ì£¼ì…
            noisy_text = self._introduce_typos(text)
        elif grammar_noise_type == 'word_order':
            # ë‹¨ì–´ ìˆœì„œ ì„ê¸°
            noisy_text = self._shuffle_words(text)
        else:  # punctuation
            # êµ¬ë‘ì  ì˜¤ë¥˜
            noisy_text = self._mess_punctuation(text)

        noisy_sample[target_field] = noisy_text
        return noisy_sample

    def _apply_semantic_noise(self, sample: pd.Series, full_df: pd.DataFrame) -> pd.Series:
        """ì˜ë¯¸ì  ë…¸ì´ì¦ˆ ì£¼ì…"""
        noisy_sample = sample.copy()

        semantic_noise_type = random.choice(['wrong_output', 'irrelevant_content'])

        if semantic_noise_type == 'wrong_output':
            # ë‹¤ë¥¸ ìƒ˜í”Œì˜ outputê³¼ ì„ê¸°
            random_idx = random.randint(0, len(full_df) - 1)
            noisy_sample['output'] = full_df.iloc[random_idx]['output']
        else:  # irrelevant_content
            # ë¬´ê´€í•œ ë‚´ìš© ì¶”ê°€
            irrelevant_phrases = [
                "By the way, did you know that cats sleep 12-16 hours a day?",
                "Speaking of pizza, I love pineapple on it.",
                "Random fact: The sky is blue because of Rayleigh scattering.",
                "Unrelated: My favorite color is purple.",
                "Fun fact: Bananas are berries but strawberries aren't."
            ]
            irrelevant_text = random.choice(irrelevant_phrases)
            noisy_sample['output'] = str(noisy_sample['output']) + " " + irrelevant_text

        return noisy_sample

    def _apply_quality_noise(self, sample: pd.Series) -> pd.Series:
        """í’ˆì§ˆ ì €í•˜ ë…¸ì´ì¦ˆ ì£¼ì…"""
        noisy_sample = sample.copy()

        quality_noise_type = random.choice(['truncate', 'duplicate', 'empty'])

        if quality_noise_type == 'truncate':
            # ì‘ë‹µì„ ì¤‘ê°„ì— ìë¥´ê¸°
            output = str(noisy_sample['output'])
            cut_point = random.randint(len(output) // 3, 2 * len(output) // 3)
            noisy_sample['output'] = output[:cut_point] + "..."
        elif quality_noise_type == 'duplicate':
            # ë‚´ìš© ì¤‘ë³µ
            output = str(noisy_sample['output'])
            sentences = output.split('. ')
            if len(sentences) > 1:
                dup_sentence = random.choice(sentences)
                noisy_sample['output'] = output + " " + dup_sentence
        else:  # empty
            # ë¹ˆ ì‘ë‹µ ë˜ëŠ” ë§¤ìš° ì§§ì€ ì‘ë‹µ
            short_responses = ["I don't know.", "Yes.", "No.", "Maybe.", ""]
            noisy_sample['output'] = random.choice(short_responses)

        return noisy_sample

    def _introduce_typos(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì— ì² ì ì˜¤ë¥˜ ì£¼ì…"""
        words = text.split()
        if not words:
            return text

        # 20% í™•ë¥ ë¡œ ê° ë‹¨ì–´ì— ì˜¤íƒ€ ì ìš©
        for i, word in enumerate(words):
            if random.random() < 0.2:
                if word.lower() in self.common_typos:
                    # ë¯¸ë¦¬ ì •ì˜ëœ ì˜¤íƒ€ ì‚¬ìš©
                    typo_word = random.choice(self.common_typos[word.lower()])
                    # ì›ë˜ ë‹¨ì–´ì˜ ëŒ€ì†Œë¬¸ì íŒ¨í„´ ìœ ì§€
                    if word.isupper():
                        words[i] = typo_word.upper()
                    elif word.istitle():
                        words[i] = typo_word.capitalize()
                    else:
                        words[i] = typo_word
                else:
                    # ëœë¤ ë¬¸ì ë³€ê²½
                    if len(word) > 2:
                        char_list = list(word)
                        rand_idx = random.randint(1, len(char_list) - 2)
                        char_list[rand_idx] = random.choice('abcdefghijklmnopqrstuvwxyz')
                        words[i] = ''.join(char_list)

        return ' '.join(words)

    def _shuffle_words(self, text: str) -> str:
        """ë¬¸ì¥ ë‚´ ë‹¨ì–´ ìˆœì„œ ì„ê¸°"""
        sentences = text.split('. ')
        shuffled_sentences = []

        for sentence in sentences:
            words = sentence.split()
            if len(words) > 3:  # ë‹¨ì–´ê°€ 3ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ ì„ê¸°
                # ì²˜ìŒê³¼ ë§ˆì§€ë§‰ ë‹¨ì–´ëŠ” ìœ ì§€í•˜ê³  ì¤‘ê°„ ë‹¨ì–´ë“¤ë§Œ ì„ê¸°
                if len(words) > 4:
                    middle_words = words[1:-1]
                    random.shuffle(middle_words)
                    shuffled_sentence = [words[0]] + middle_words + [words[-1]]
                else:
                    shuffled_sentence = words[:]
                    random.shuffle(shuffled_sentence)
                shuffled_sentences.append(' '.join(shuffled_sentence))
            else:
                shuffled_sentences.append(sentence)

        return '. '.join(shuffled_sentences)

    def _mess_punctuation(self, text: str) -> str:
        """êµ¬ë‘ì  ì˜¤ë¥˜ ì£¼ì…"""
        # êµ¬ë‘ì  ì œê±° ë˜ëŠ” ì˜ëª»ëœ êµ¬ë‘ì  ì¶”ê°€
        noise_operations = [
            lambda x: x.replace('.', ''),  # ë§ˆì¹¨í‘œ ì œê±°
            lambda x: x.replace(',', ''),  # ì‰¼í‘œ ì œê±°
            lambda x: x.replace('.', '!'),  # ë§ˆì¹¨í‘œë¥¼ ëŠë‚Œí‘œë¡œ
            lambda x: x.replace('?', '.'),  # ë¬¼ìŒí‘œë¥¼ ë§ˆì¹¨í‘œë¡œ
            lambda x: x + '???',  # ë¬¼ìŒí‘œ ê³¼ë‹¤ ì‚¬ìš©
        ]

        operation = random.choice(noise_operations)
        return operation(text)


# ë…¸ì´ì¦ˆ ë¶„ì„ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def compare_samples(original_df: pd.DataFrame, noisy_df: pd.DataFrame,
                    noisy_indices: List[int], num_examples: int = 5):
    """ë…¸ì´ì¦ˆ ì£¼ì… ì „í›„ ìƒ˜í”Œ ë¹„êµ"""
    print(f"\n=== ë…¸ì´ì¦ˆ ì£¼ì… ì „í›„ ë¹„êµ ({num_examples}ê°œ ìƒ˜í”Œ) ===")

    sample_indices = random.sample(noisy_indices, min(num_examples, len(noisy_indices)))

    for i, idx in enumerate(sample_indices):
        print(f"\n[ì˜ˆì‹œ {i + 1}] ì¸ë±ìŠ¤: {idx}")
        print("=" * 60)

        print("ğŸ”µ ì›ë³¸:")
        print(f"Instruction: {original_df.iloc[idx]['instruction']}")
        print(f"Output: {original_df.iloc[idx]['output']}")

        print("\nğŸ”´ ë…¸ì´ì¦ˆ ì£¼ì… í›„:")
        print(f"Instruction: {noisy_df.iloc[idx]['instruction']}")
        print(f"Output: {noisy_df.iloc[idx]['output']}")
        print("-" * 60)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    from data_loader import AlpacaDataLoader

    print("=== ë…¸ì´ì¦ˆ ì£¼ì… í…ŒìŠ¤íŠ¸ ===")

    # ë°ì´í„° ë¡œë“œ
    loader = AlpacaDataLoader()
    df = loader.load_alpaca_dataset(subset_size=50)  # ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸

    if df is not None:
        # ë…¸ì´ì¦ˆ ì£¼ì…ê¸° ìƒì„±
        injector = NoiseInjector()

        # ë…¸ì´ì¦ˆ ì£¼ì… (20% ë¹„ìœ¨)
        noisy_df, noisy_indices = injector.inject_noise(df, noise_ratio=0.2)

        # ê²°ê³¼ ë¹„êµ
        compare_samples(df, noisy_df, noisy_indices, num_examples=3)

        # ì €ì¥
        loader.save_dataset(noisy_df, "alpaca_noisy_test.json")

        print(f"\nâœ… ë…¸ì´ì¦ˆ ì£¼ì… ì™„ë£Œ!")
        print(f"ë…¸ì´ì¦ˆê°€ ì£¼ì…ëœ ìƒ˜í”Œ ìˆ˜: {len(noisy_indices)}")