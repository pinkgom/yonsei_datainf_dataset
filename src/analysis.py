"""
DataInf ë‹¤ì¤‘ ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹(alpaca, gsm8k, sst2, mrpc)ìœ¼ë¡œ ìƒì„±ëœ
ë…¸ì´ì¦ˆ ë°ì´í„°ì…‹ë“¤ì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import time
import random
import re
from .noise_injection import analyze_noise_distribution, compare_samples


def print_separator(title="", char="=", length=60):
    """êµ¬ë¶„ì„  ì¶œë ¥ í•¨ìˆ˜"""
    if title:
        title_line = f" {title} "
        padding = (length - len(title_line)) // 2
        line = char * padding + title_line + char * padding
        if len(line) < length:
            line += char
    else:
        line = char * length
    print(line)


def run_quality_analysis():
    """ë‹¤ì¤‘ ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„ ëª¨ë“œ"""
    print_separator("ë‹¤ì¤‘ ë°ì´í„°ì…‹ í’ˆì§ˆ ë¶„ì„", "=", 70)

    from .data_loader import MultiDatasetLoader
    loader = MultiDatasetLoader()

    # ê¸°ì¡´ íŒŒì¼ë“¤ ì°¾ê¸° ë° ë°ì´í„°ì…‹ë³„ ë¶„ë¥˜
    data_files = [f for f in os.listdir("./data") if f.endswith(".json")]

    if not data_files:
        print("ë¶„ì„í•  ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë°ì´í„°ì…‹ë³„ë¡œ íŒŒì¼ ë¶„ë¥˜
    datasets = {
        'alpaca': [],
        'gsm8k': [],
        'sst2': [],
        'mrpc': []
    }

    other_files = []

    for file in data_files:
        classified = False
        for dataset_name in datasets.keys():
            if file.startswith(dataset_name):
                datasets[dataset_name].append(file)
                classified = True
                break
        if not classified:
            other_files.append(file)

    # ì „ì²´ íŒŒì¼ í˜„í™©
    total_files = sum(len(files) for files in datasets.values()) + len(other_files)
    print(f"ë°œê²¬ëœ ë°ì´í„° íŒŒì¼: {total_files}ê°œ")

    for dataset_name, files in datasets.items():
        if files:
            original_count = len([f for f in files if "original" in f])
            experiment_count = len(files) - original_count
            print(f"   - {dataset_name.upper()}: {len(files)}ê°œ (ì›ë³¸: {original_count}, ì‹¤í—˜: {experiment_count})")

    if other_files:
        print(f"   - ê¸°íƒ€: {len(other_files)}ê°œ")

    # ë¶„ì„ ì˜µì…˜ ì„ íƒ
    print(f"\në¶„ì„ ì˜µì…˜:")
    print("1. ì „ì²´ íŒŒì¼ ê¸°ë³¸ ì •ë³´")
    print("2. ë°ì´í„°ì…‹ë³„ ìƒì„¸ ë¶„ì„")
    print("3. ë°ì´í„°ì…‹ê°„ ë…¸ì´ì¦ˆ íš¨ê³¼ ë¹„êµ")
    print("4. íŠ¹ì • ë°ì´í„°ì…‹ ì§‘ì¤‘ ë¶„ì„")
    print("5. ë¼ë²¨ ë³´ì¡´ ê²€ì¦ (Classification ë°ì´í„°ì…‹)")
    print("6. ì „ì²´ ì¢…í•© ë¶„ì„")

    choice = input("\nì„ íƒí•˜ì„¸ìš” (1-6, ë˜ëŠ” Enterë¡œ ê¸°ë³¸ ì •ë³´): ").strip()

    if choice == "1" or choice == "":
        analyze_all_files_basic_info(datasets, other_files)
    elif choice == "2":
        analyze_by_dataset_detailed(datasets, loader)
    elif choice == "3":
        analyze_cross_dataset_comparison(datasets, loader)
    elif choice == "4":
        analyze_specific_dataset(datasets, loader)
    elif choice == "5":
        analyze_label_preservation(datasets, loader)
    elif choice == "6":
        run_comprehensive_multi_dataset_analysis(datasets, loader)
    else:
        print("ì˜¬ë°”ë¥´ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤.")


def analyze_all_files_basic_info(datasets, other_files):
    """ì „ì²´ íŒŒì¼ ê¸°ë³¸ ì •ë³´ ë¶„ì„"""
    print_separator("ì „ì²´ íŒŒì¼ ê¸°ë³¸ ì •ë³´", "-")

    # í…Œì´ë¸” í—¤ë”
    print(f"{'íŒŒì¼ëª…':<60} {'ë°ì´í„°ì…‹':<10} {'íƒ€ì…':<12} {'ìƒ˜í”Œìˆ˜':<8} {'ë…¸ì´ì¦ˆ%':<8} {'ì „ëµ':<15} {'í¬ê¸°(MB)':<10}")
    print("=" * 130)

    # ë°ì´í„°ì…‹ë³„ íŒŒì¼ ì •ë³´
    for dataset_name, files in datasets.items():
        if not files:
            continue

        for file in sorted(files):
            info = parse_filename_info(file, dataset_name)
            filepath = os.path.join("./data", file)
            file_size = os.path.getsize(filepath) / (1024 * 1024) if os.path.exists(filepath) else 0

            print(f"{file:<60} {dataset_name:<10} {info['type']:<12} {info['samples']:<8} "
                  f"{info['noise_percent']:<8} {info['strategy']:<15} {file_size:<10.1f}")

    # ê¸°íƒ€ íŒŒì¼ë“¤
    if other_files:
        print("\nê¸°íƒ€ íŒŒì¼ë“¤:")
        for file in sorted(other_files):
            filepath = os.path.join("./data", file)
            file_size = os.path.getsize(filepath) / (1024 * 1024) if os.path.exists(filepath) else 0
            print(f"{file:<60} {'unknown':<10} {'unknown':<12} {'N/A':<8} "
                  f"{'N/A':<8} {'N/A':<15} {file_size:<10.1f}")


def analyze_by_dataset_detailed(datasets, loader):
    """ë°ì´í„°ì…‹ë³„ ìƒì„¸ ë¶„ì„"""
    print_separator("ë°ì´í„°ì…‹ë³„ ìƒì„¸ ë¶„ì„", "-")

    for dataset_name, files in datasets.items():
        if not files:
            continue

        print(f"\nğŸ” {dataset_name.upper()} ë°ì´í„°ì…‹ ë¶„ì„")
        print("=" * 50)

        # ì›ë³¸ íŒŒì¼ê³¼ ë…¸ì´ì¦ˆ íŒŒì¼ ë¶„ë¦¬
        original_files = [f for f in files if "original" in f]
        noise_files = [f for f in files if "original" not in f]

        print(f"ì›ë³¸ íŒŒì¼: {len(original_files)}ê°œ")
        print(f"ë…¸ì´ì¦ˆ íŒŒì¼: {len(noise_files)}ê°œ")

        if not original_files:
            print("   âš ï¸  ì›ë³¸ íŒŒì¼ì´ ì—†ì–´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        # ëŒ€í‘œ ì›ë³¸ íŒŒì¼ (ê°€ì¥ í° ê²ƒ)
        main_original = max(original_files, key=lambda f: os.path.getsize(os.path.join("./data", f)))
        print(f"ë¶„ì„ ê¸°ì¤€ ì›ë³¸: {main_original}")

        # ì›ë³¸ ë°ì´í„° ë¡œë“œ
        original_df = loader.load_saved_dataset(main_original)
        if original_df is None:
            print("   âš ï¸  ì›ë³¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
            continue

        print(f"ì›ë³¸ ë°ì´í„°: {len(original_df):,}ê°œ ìƒ˜í”Œ")

        # ë°ì´í„°ì…‹ë³„ íŠ¹ì„± ë¶„ì„
        analyze_dataset_characteristics(original_df, dataset_name)

        # ë…¸ì´ì¦ˆ íŒŒì¼ë“¤ ë¶„ì„
        if noise_files:
            print(f"\nğŸ“Š ë…¸ì´ì¦ˆ íŒŒì¼ ë¶„ì„:")

            for noise_file in sorted(noise_files)[:3]:  # ìµœëŒ€ 3ê°œë§Œ
                print(f"\n   ë¶„ì„ ì¤‘: {noise_file}")

                noisy_df = loader.load_saved_dataset(noise_file)
                if noisy_df is None:
                    continue

                # ë…¸ì´ì¦ˆ ì¸ë±ìŠ¤ ì¶”ì •
                noisy_indices = estimate_noisy_indices(original_df, noisy_df, dataset_name)

                # ë¶„ì„ ì‹¤í–‰
                analysis = analyze_noise_distribution(noisy_df, original_df, noisy_indices, dataset_name)

                # ê²°ê³¼ ì¶œë ¥
                print(f"      - ì „ì²´ ìƒ˜í”Œ: {analysis['total_samples']:,}ê°œ")
                print(f"      - ì¶”ì • ë…¸ì´ì¦ˆ: {len(noisy_indices):,}ê°œ ({len(noisy_indices)/len(original_df)*100:.1f}%)")
                print(f"      - ì‹¤ì œ ë³€ê²½: {analysis['actual_changes']:,}ê°œ")
                print(f"      - í‰ê·  ê¸¸ì´ ë³€í™”: {analysis['avg_length_change']:.1f} ë¬¸ì")


def analyze_cross_dataset_comparison(datasets, loader):
    """ë°ì´í„°ì…‹ê°„ ë…¸ì´ì¦ˆ íš¨ê³¼ ë¹„êµ"""
    print_separator("ë°ì´í„°ì…‹ê°„ ë…¸ì´ì¦ˆ íš¨ê³¼ ë¹„êµ", "-")

    # ê° ë°ì´í„°ì…‹ì˜ 20% balanced ë…¸ì´ì¦ˆ íŒŒì¼ ì°¾ê¸°
    comparison_files = {}

    for dataset_name, files in datasets.items():
        if not files:
            continue

        # 20% balanced íŒŒì¼ ì°¾ê¸°
        target_files = [f for f in files if "20percent" in f and "balanced" in f and "original" not in f]
        if target_files:
            comparison_files[dataset_name] = target_files[0]

    if len(comparison_files) < 2:
        print("ë¹„êµí•  ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. (ê° ë°ì´í„°ì…‹ì— 20% balanced ë…¸ì´ì¦ˆ íŒŒì¼ í•„ìš”)")
        return

    print(f"ë¹„êµ ëŒ€ìƒ: {list(comparison_files.keys())}")
    print(f"ë¶„ì„ ì¡°ê±´: 20% balanced ë…¸ì´ì¦ˆ")

    # ë¹„êµ ê²°ê³¼ ìˆ˜ì§‘
    comparison_results = {}

    for dataset_name, noise_file in comparison_files.items():
        print(f"\nğŸ“Š {dataset_name.upper()} ë¶„ì„ ì¤‘...")

        # ì›ë³¸ íŒŒì¼ ì°¾ê¸°
        original_files = [f for f in datasets[dataset_name] if "original" in f]
        if not original_files:
            continue

        original_file = original_files[0]

        # ë°ì´í„° ë¡œë“œ
        original_df = loader.load_saved_dataset(original_file)
        noisy_df = loader.load_saved_dataset(noise_file)

        if original_df is None or noisy_df is None:
            continue

        # ë…¸ì´ì¦ˆ ì¸ë±ìŠ¤ ì¶”ì • ë° ë¶„ì„
        noisy_indices = estimate_noisy_indices(original_df, noisy_df, dataset_name)
        analysis = analyze_noise_distribution(noisy_df, original_df, noisy_indices, dataset_name)

        comparison_results[dataset_name] = {
            'total_samples': len(original_df),
            'target_noise_samples': len(noisy_indices),
            'actual_changes': analysis['actual_changes'],
            'actual_noise_ratio': analysis['actual_noise_ratio'],
            'avg_length_change': analysis['avg_length_change'],
            'field_changes': analysis['field_changes']
        }

    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print_separator("ë°ì´í„°ì…‹ê°„ ë¹„êµ ê²°ê³¼", "-")

    print(f"{'ë°ì´í„°ì…‹':<10} {'ì´ ìƒ˜í”Œ':<10} {'ë…¸ì´ì¦ˆ ëŒ€ìƒ':<12} {'ì‹¤ì œ ë³€ê²½':<12} {'ì‹¤ì œ ë¹„ìœ¨':<10} {'ê¸¸ì´ ë³€í™”':<10}")
    print("-" * 70)

    for dataset_name, result in comparison_results.items():
        print(f"{dataset_name:<10} {result['total_samples']:<10,} {result['target_noise_samples']:<12,} "
              f"{result['actual_changes']:<12,} {result['actual_noise_ratio']*100:<9.1f}% {result['avg_length_change']:<10.1f}")

    # ë…¸ì´ì¦ˆ íš¨ê³¼ì„± ë¶„ì„
    print(f"\nğŸ“ˆ ë…¸ì´ì¦ˆ íš¨ê³¼ì„± ìˆœìœ„:")
    sorted_results = sorted(comparison_results.items(),
                          key=lambda x: x[1]['actual_noise_ratio'], reverse=True)

    for i, (dataset_name, result) in enumerate(sorted_results, 1):
        effectiveness = result['actual_changes'] / result['target_noise_samples'] * 100
        print(f"{i}. {dataset_name.upper()}: {effectiveness:.1f}% íš¨ê³¼ì„± "
              f"(ëª©í‘œ {result['target_noise_samples']:,}ê°œ â†’ ì‹¤ì œ {result['actual_changes']:,}ê°œ)")


def analyze_specific_dataset(datasets, loader):
    """íŠ¹ì • ë°ì´í„°ì…‹ ì§‘ì¤‘ ë¶„ì„"""
    print_separator("íŠ¹ì • ë°ì´í„°ì…‹ ì§‘ì¤‘ ë¶„ì„", "-")

    # ë°ì´í„°ì…‹ ì„ íƒ
    available_datasets = [name for name, files in datasets.items() if files]

    if not available_datasets:
        print("ë¶„ì„í•  ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ë¶„ì„ ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
    for i, dataset_name in enumerate(available_datasets, 1):
        file_count = len(datasets[dataset_name])
        print(f"{i}. {dataset_name.upper()} ({file_count}ê°œ íŒŒì¼)")

    try:
        choice = int(input("ì„ íƒí•˜ì„¸ìš”: ")) - 1
        selected_dataset = available_datasets[choice]
    except (ValueError, IndexError):
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return

    print(f"\nğŸ¯ {selected_dataset.upper()} ì§‘ì¤‘ ë¶„ì„")
    print("=" * 50)

    files = datasets[selected_dataset]

    # íŒŒì¼ ë¶„ë¥˜
    original_files = [f for f in files if "original" in f]
    demo_files = [f for f in files if "demo" in f and "original" not in f]
    full_files = [f for f in files if "full" in f and "original" not in f]

    print(f"ì›ë³¸ íŒŒì¼: {len(original_files)}ê°œ")
    print(f"ë°ëª¨ íŒŒì¼: {len(demo_files)}ê°œ")
    print(f"ì „ì²´ íŒŒì¼: {len(full_files)}ê°œ")

    # ì›ë³¸ ë°ì´í„° íŠ¹ì„±
    if original_files:
        main_original = original_files[0]
        original_df = loader.load_saved_dataset(main_original)
        if original_df is not None:
            print(f"\nğŸ“‹ ë°ì´í„°ì…‹ íŠ¹ì„±:")
            analyze_dataset_characteristics(original_df, selected_dataset)

    # ë…¸ì´ì¦ˆ ì „ëµë³„ ë¶„ì„
    if demo_files or full_files:
        print(f"\nğŸ“Š ë…¸ì´ì¦ˆ ì „ëµë³„ íš¨ê³¼:")

        target_files = demo_files if demo_files else full_files
        strategy_results = {}

        for file in target_files:
            info = parse_filename_info(file, selected_dataset)
            strategy = info['strategy']
            noise_percent = info['noise_percent']

            if strategy not in strategy_results:
                strategy_results[strategy] = []

            strategy_results[strategy].append({
                'file': file,
                'noise_percent': noise_percent
            })

        for strategy, file_info_list in strategy_results.items():
            print(f"\n   {strategy} ì „ëµ:")
            for file_info in file_info_list:
                print(f"      - {file_info['file']} (ë…¸ì´ì¦ˆ: {file_info['noise_percent']})")


def analyze_label_preservation(datasets, loader):
    """ë¼ë²¨ ë³´ì¡´ ê²€ì¦ (Classification ë°ì´í„°ì…‹)"""
    print_separator("ë¼ë²¨ ë³´ì¡´ ê²€ì¦", "-")

    # Classification ë°ì´í„°ì…‹ë§Œ ì„ íƒ
    classification_datasets = ['gsm8k', 'sst2', 'mrpc']
    available_classification = {name: files for name, files in datasets.items()
                              if name in classification_datasets and files}

    if not available_classification:
        print("ë¼ë²¨ ë³´ì¡´ ê²€ì¦ì´ ê°€ëŠ¥í•œ Classification ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("í•„ìš”í•œ ë°ì´í„°ì…‹: gsm8k, sst2, mrpc")
        return

    print(f"ê²€ì¦ ëŒ€ìƒ ë°ì´í„°ì…‹: {list(available_classification.keys())}")

    from .noise_injection import MultiDatasetNoiseInjector
    injector = MultiDatasetNoiseInjector()

    for dataset_name, files in available_classification.items():
        print(f"\nğŸ” {dataset_name.upper()} ë¼ë²¨ ë³´ì¡´ ê²€ì¦")
        print("-" * 40)

        config = injector.dataset_configs.get(dataset_name, {})
        label_columns = config.get('label_columns', [])

        if not label_columns:
            print("   ë¼ë²¨ ì»¬ëŸ¼ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ")
            continue

        print(f"   ê²€ì¦ ë¼ë²¨: {label_columns}")

        # ì›ë³¸ íŒŒì¼ê³¼ ë…¸ì´ì¦ˆ íŒŒì¼ ì°¾ê¸°
        original_files = [f for f in files if "original" in f]
        noise_files = [f for f in files if "original" not in f]

        if not original_files or not noise_files:
            print("   ì›ë³¸ ë˜ëŠ” ë…¸ì´ì¦ˆ íŒŒì¼ì´ ì—†ìŒ")
            continue

        # ëŒ€í‘œ íŒŒì¼ë“¤ë¡œ ê²€ì¦
        original_file = original_files[0]

        print(f"   ì›ë³¸ íŒŒì¼: {original_file}")

        original_df = loader.load_saved_dataset(original_file)
        if original_df is None:
            continue

        label_preservation_results = {}

        for noise_file in noise_files[:3]:  # ìµœëŒ€ 3ê°œ íŒŒì¼ë§Œ ê²€ì¦
            print(f"\n   ê²€ì¦ ì¤‘: {noise_file}")

            noisy_df = loader.load_saved_dataset(noise_file)
            if noisy_df is None:
                continue

            # ë¼ë²¨ ë³€ê²½ ì—¬ë¶€ í™•ì¸
            label_changes = {}
            total_samples = min(len(original_df), len(noisy_df))

            for label_col in label_columns:
                if label_col in original_df.columns and label_col in noisy_df.columns:
                    changes = 0
                    for i in range(total_samples):
                        if original_df.iloc[i][label_col] != noisy_df.iloc[i][label_col]:
                            changes += 1
                    label_changes[label_col] = changes
                else:
                    label_changes[label_col] = "ì»¬ëŸ¼ ì—†ìŒ"

            label_preservation_results[noise_file] = label_changes

            # ê²°ê³¼ ì¶œë ¥
            for label_col, changes in label_changes.items():
                if isinstance(changes, int):
                    if changes == 0:
                        print(f"      âœ… {label_col}: ì™„ë²½ ë³´ì¡´ (ë³€ê²½ 0ê°œ)")
                    else:
                        print(f"      âŒ {label_col}: {changes}ê°œ ë³€ê²½ë¨ ({changes/total_samples*100:.2f}%)")
                else:
                    print(f"      âš ï¸  {label_col}: {changes}")

        # ìš”ì•½
        if label_preservation_results:
            print(f"\n   ğŸ“Š {dataset_name.upper()} ë¼ë²¨ ë³´ì¡´ ìš”ì•½:")
            perfect_preservation = 0
            total_files = len(label_preservation_results)

            for noise_file, label_changes in label_preservation_results.items():
                all_preserved = all(changes == 0 for changes in label_changes.values()
                                  if isinstance(changes, int))
                if all_preserved:
                    perfect_preservation += 1

            print(f"      ì™„ë²½ ë³´ì¡´ íŒŒì¼: {perfect_preservation}/{total_files}ê°œ")
            if perfect_preservation == total_files:
                print(f"      âœ… ëª¨ë“  íŒŒì¼ì—ì„œ ë¼ë²¨ì´ ì™„ë²½í•˜ê²Œ ë³´ì¡´ë¨!")
            else:
                print(f"      âš ï¸  ì¼ë¶€ íŒŒì¼ì—ì„œ ë¼ë²¨ ë³€ê²½ ë°œìƒ")


def run_comprehensive_multi_dataset_analysis(datasets, loader):
    """ì „ì²´ ì¢…í•© ë¶„ì„ (ë‹¤ì¤‘ ë°ì´í„°ì…‹)"""
    print_separator("ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì¢…í•© ë¶„ì„", "=", 70)

    print("ì¢…í•© ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("   ì´ ë¶„ì„ì€ ëª¨ë“  ë°ì´í„°ì…‹ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ë©° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    confirm = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if confirm != 'y':
        return

    # 1. ì „ì²´ í˜„í™©
    print("\n" + "=" * 50)
    print("1. ì „ì²´ í˜„í™©")
    print("=" * 50)
    analyze_all_files_basic_info(datasets, [])

    # 2. ë°ì´í„°ì…‹ë³„ ìš”ì•½
    print("\n" + "=" * 50)
    print("2. ë°ì´í„°ì…‹ë³„ ìš”ì•½")
    print("=" * 50)

    total_datasets = 0
    total_files = 0

    for dataset_name, files in datasets.items():
        if files:
            total_datasets += 1
            total_files += len(files)

            original_count = len([f for f in files if "original" in f])
            noise_count = len(files) - original_count

            print(f"{dataset_name.upper()}:")
            print(f"   - ì´ íŒŒì¼: {len(files)}ê°œ")
            print(f"   - ì›ë³¸: {original_count}ê°œ")
            print(f"   - ë…¸ì´ì¦ˆ: {noise_count}ê°œ")

            # ë…¸ì´ì¦ˆ ì „ëµ ë¶„í¬
            strategies = set()
            noise_ratios = set()

            for file in files:
                if "original" not in file:
                    info = parse_filename_info(file, dataset_name)
                    if info['strategy'] != 'N/A':
                        strategies.add(info['strategy'])
                    if info['noise_percent'] != 'N/A':
                        noise_ratios.add(info['noise_percent'])

            if strategies:
                print(f"   - í…ŒìŠ¤íŠ¸ëœ ì „ëµ: {', '.join(strategies)}")
            if noise_ratios:
                print(f"   - í…ŒìŠ¤íŠ¸ëœ ë…¸ì´ì¦ˆ ë¹„ìœ¨: {', '.join(sorted(noise_ratios))}")
            print()

    # 3. ë°ì´í„°ì…‹ê°„ ë¹„êµ
    if total_datasets > 1:
        print("\n" + "=" * 50)
        print("3. ë°ì´í„°ì…‹ê°„ ë¹„êµ")
        print("=" * 50)
        analyze_cross_dataset_comparison(datasets, loader)

    # 4. ë¼ë²¨ ë³´ì¡´ ê²€ì¦
    print("\n" + "=" * 50)
    print("4. ë¼ë²¨ ë³´ì¡´ ê²€ì¦")
    print("=" * 50)
    analyze_label_preservation(datasets, loader)

    # 5. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 50)
    print("5. ìµœì¢… ìš”ì•½")
    print("=" * 50)

    print(f"ì „ì²´ í˜„í™©:")
    print(f"  - í™œì„± ë°ì´í„°ì…‹: {total_datasets}ê°œ")
    print(f"  - ì´ íŒŒì¼ ìˆ˜: {total_files}ê°œ")

    # DataInf ì‹¤í—˜ ì¤€ë¹„ë„ ì²´í¬
    print(f"\nDataInf ì‹¤í—˜ ì¤€ë¹„ë„:")

    ready_datasets = []

    for dataset_name, files in datasets.items():
        if not files:
            continue

        has_original = any("original" in f for f in files)
        has_noise = any("original" not in f for f in files)
        has_multiple_ratios = len(set(parse_filename_info(f, dataset_name)['noise_percent']
                                    for f in files if parse_filename_info(f, dataset_name)['noise_percent'] != 'N/A')) > 1

        dataset_readiness = sum([has_original, has_noise, has_multiple_ratios])

        print(f"  {dataset_name.upper()}:")
        print(f"    - ì›ë³¸ ë°ì´í„°: {'âœ…' if has_original else 'âŒ'}")
        print(f"    - ë…¸ì´ì¦ˆ ë°ì´í„°: {'âœ…' if has_noise else 'âŒ'}")
        print(f"    - ë‹¤ì–‘í•œ ë¹„ìœ¨: {'âœ…' if has_multiple_ratios else 'âŒ'}")

        if dataset_readiness == 3:
            print(f"    ğŸ“Š ì¤€ë¹„ë„: ì™„ë²½ (3/3) - ì‹¤í—˜ ê°€ëŠ¥!")
            ready_datasets.append(dataset_name)
        elif dataset_readiness == 2:
            print(f"    ğŸ“Š ì¤€ë¹„ë„: ì–‘í˜¸ (2/3) - ì¶”ê°€ ë°ì´í„° ê¶Œì¥")
        else:
            print(f"    ğŸ“Š ì¤€ë¹„ë„: ë¶€ì¡± ({dataset_readiness}/3) - ë°ì´í„° ìƒì„± í•„ìš”")

    if ready_datasets:
        print(f"\nğŸ‰ ì‹¤í—˜ ì¤€ë¹„ ì™„ë£Œëœ ë°ì´í„°ì…‹: {', '.join(ready_datasets)}")
        print(f"   ì´ì œ LoRA í•™ìŠµ ë‹´ë‹¹ìì—ê²Œ ë°ì´í„°ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print(f"\nâš ï¸  ì•„ì§ ì‹¤í—˜ ì¤€ë¹„ê°€ ì™„ë£Œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"\në‹¤ì¤‘ ë°ì´í„°ì…‹ ì¢…í•© ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


# í—¬í¼ í•¨ìˆ˜ë“¤

def parse_filename_info(filename, dataset_name):
    """ë°ì´í„°ì…‹ë³„ íŒŒì¼ëª… ì •ë³´ ì¶”ì¶œ"""
    info = {
        'type': 'unknown',
        'samples': 'N/A',
        'noise_percent': 'N/A',
        'strategy': 'N/A'
    }

    # íƒ€ì… íŒë³„
    if 'original' in filename:
        info['type'] = 'original'
    elif 'demo' in filename:
        info['type'] = 'demo'
    elif 'full' in filename:
        info['type'] = 'experiment'

    # ìƒ˜í”Œ ìˆ˜ ì¶”ì¶œ
    sample_match = re.search(r'_(\d+)\.json$', filename)
    if sample_match:
        info['samples'] = sample_match.group(1)

    # ë…¸ì´ì¦ˆ ë¹„ìœ¨ ì¶”ì¶œ
    noise_match = re.search(r'_(\d+)percent_', filename)
    if noise_match:
        info['noise_percent'] = f"{noise_match.group(1)}%"

    # ì „ëµ ì¶”ì¶œ
    if 'balanced' in filename:
        info['strategy'] = 'balanced'
    elif 'grammar' in filename:
        info['strategy'] = 'grammar_heavy'
    elif 'semantic' in filename:
        info['strategy'] = 'semantic_heavy'

    return info


def estimate_noisy_indices(original_df, noisy_df, dataset_name):
    """ë°ì´í„°ì…‹ë³„ ë…¸ì´ì¦ˆ ì¸ë±ìŠ¤ ì¶”ì •"""
    from .noise_injection import MultiDatasetNoiseInjector

    injector = MultiDatasetNoiseInjector()
    config = injector.dataset_configs.get(dataset_name, {})
    text_columns = config.get('text_columns', [])

    if not text_columns:
        # fallback: ëª¨ë“  ë¬¸ìì—´ ì»¬ëŸ¼ ë¹„êµ
        text_columns = [col for col in original_df.columns if original_df[col].dtype == 'object']

    noisy_indices = []
    max_samples = min(len(original_df), len(noisy_df))

    for i in range(max_samples):
        for col in text_columns:
            if col in original_df.columns and col in noisy_df.columns:
                if str(original_df.iloc[i][col]) != str(noisy_df.iloc[i][col]):
                    noisy_indices.append(i)
                    break

    return noisy_indices


def analyze_dataset_characteristics(df, dataset_name):
    """ë°ì´í„°ì…‹ë³„ íŠ¹ì„± ë¶„ì„"""
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(df):,}ê°œ")
    print(f"   ì»¬ëŸ¼: {list(df.columns)}")

    if dataset_name == 'alpaca':
        if 'instruction' in df.columns:
            inst_lengths = df['instruction'].str.len()
            print(f"   Instruction ê¸¸ì´ - í‰ê· : {inst_lengths.mean():.1f}, ë²”ìœ„: {inst_lengths.min()}-{inst_lengths.max()}")
        if 'output' in df.columns:
            out_lengths = df['output'].str.len()
            print(f"   Output ê¸¸ì´ - í‰ê· : {out_lengths.mean():.1f}, ë²”ìœ„: {out_lengths.min()}-{out_lengths.max()}")

    elif dataset_name == 'gsm8k':
        if 'question' in df.columns:
            q_lengths = df['question'].str.len()
            print(f"   Question ê¸¸ì´ - í‰ê· : {q_lengths.mean():.1f}, ë²”ìœ„: {q_lengths.min()}-{q_lengths.max()}")
        if 'answer' in df.columns:
            a_lengths = df['answer'].str.len()
            print(f"   Answer ê¸¸ì´ - í‰ê· : {a_lengths.mean():.1f}, ë²”ìœ„: {a_lengths.min()}-{a_lengths.max()}")

    elif dataset_name == 'sst2':
        if 'sentence' in df.columns:
            sent_lengths = df['sentence'].str.len()
            print(f"   Sentence ê¸¸ì´ - í‰ê· : {sent_lengths.mean():.1f}, ë²”ìœ„: {sent_lengths.min()}-{sent_lengths.max()}")
        if 'label' in df.columns:
            label_dist = df['label'].value_counts()
            print(f"   ë¼ë²¨ ë¶„í¬: {dict(label_dist)}")

    elif dataset_name == 'mrpc':
        if 'sentence1' in df.columns and 'sentence2' in df.columns:
            s1_lengths = df['sentence1'].str.len()
            s2_lengths = df['sentence2'].str.len()
            print(f"   Sentence1 ê¸¸ì´ - í‰ê· : {s1_lengths.mean():.1f}")
            print(f"   Sentence2 ê¸¸ì´ - í‰ê· : {s2_lengths.mean():.1f}")
        if 'label' in df.columns:
            label_dist = df['label'].value_counts()
            print(f"   ë¼ë²¨ ë¶„í¬: {dict(label_dist)}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì§ì ‘ ì‹¤í–‰ì‹œ)
    print("=== Multi-Dataset Analysis ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ===")
    run_quality_analysis()